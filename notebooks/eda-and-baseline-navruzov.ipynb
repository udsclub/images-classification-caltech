{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for NN composition (TODO)\n",
    "#from keras import backend as K\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Conv2D, Dropout, BatchNormalization, Dense, MaxPool2D, Flatten\n",
    "#from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# for image preprocessing/plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage \n",
    "from scipy import ndimage, misc\n",
    "\n",
    "# for os manipulations\n",
    "import os\n",
    "import errno\n",
    "import shutil\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading files from 001.ak47...\n",
      "reading files from 002.american-flag...\n",
      "reading files from 003.backpack...\n",
      "reading files from 004.baseball-bat...\n",
      "reading files from 005.baseball-glove...\n",
      "reading files from 006.basketball-hoop...\n",
      "reading files from 007.bat...\n",
      "reading files from 008.bathtub...\n",
      "reading files from 009.bear...\n",
      "reading files from 010.beer-mug...\n",
      "reading files from 011.billiards...\n",
      "reading files from 012.binoculars...\n",
      "reading files from 013.birdbath...\n",
      "reading files from 014.blimp...\n",
      "reading files from 015.bonsai-101...\n",
      "reading files from 016.boom-box...\n",
      "reading files from 017.bowling-ball...\n",
      "reading files from 018.bowling-pin...\n",
      "reading files from 019.boxing-glove...\n",
      "reading files from 020.brain-101...\n",
      "reading files from 021.breadmaker...\n",
      "reading files from 022.buddha-101...\n",
      "reading files from 023.bulldozer...\n",
      "reading files from 024.butterfly...\n",
      "reading files from 025.cactus...\n",
      "reading files from 026.cake...\n",
      "reading files from 027.calculator...\n",
      "reading files from 028.camel...\n",
      "reading files from 029.cannon...\n",
      "reading files from 030.canoe...\n",
      "reading files from 031.car-tire...\n",
      "reading files from 032.cartman...\n",
      "reading files from 033.cd...\n",
      "reading files from 034.centipede...\n",
      "reading files from 035.cereal-box...\n",
      "reading files from 036.chandelier-101...\n",
      "reading files from 037.chess-board...\n",
      "reading files from 038.chimp...\n",
      "reading files from 039.chopsticks...\n",
      "reading files from 040.cockroach...\n",
      "reading files from 041.coffee-mug...\n",
      "reading files from 042.coffin...\n",
      "reading files from 043.coin...\n",
      "reading files from 044.comet...\n",
      "reading files from 045.computer-keyboard...\n",
      "reading files from 046.computer-monitor...\n",
      "reading files from 047.computer-mouse...\n",
      "reading files from 048.conch...\n",
      "reading files from 049.cormorant...\n",
      "reading files from 050.covered-wagon...\n",
      "reading files from 051.cowboy-hat...\n",
      "reading files from 052.crab-101...\n",
      "reading files from 053.desk-globe...\n",
      "reading files from 054.diamond-ring...\n",
      "reading files from 055.dice...\n",
      "reading files from 056.dog...\n",
      "reading files from 057.dolphin-101...\n",
      "reading files from 058.doorknob...\n",
      "reading files from 059.drinking-straw...\n",
      "reading files from 060.duck...\n",
      "reading files from 061.dumb-bell...\n",
      "reading files from 062.eiffel-tower...\n",
      "reading files from 063.electric-guitar-101...\n",
      "reading files from 064.elephant-101...\n",
      "reading files from 065.elk...\n",
      "reading files from 066.ewer-101...\n",
      "reading files from 067.eyeglasses...\n",
      "reading files from 068.fern...\n",
      "reading files from 069.fighter-jet...\n",
      "reading files from 070.fire-extinguisher...\n",
      "reading files from 071.fire-hydrant...\n",
      "reading files from 072.fire-truck...\n",
      "reading files from 073.fireworks...\n",
      "reading files from 074.flashlight...\n",
      "reading files from 075.floppy-disk...\n",
      "reading files from 076.football-helmet...\n",
      "reading files from 077.french-horn...\n",
      "reading files from 078.fried-egg...\n",
      "reading files from 079.frisbee...\n",
      "reading files from 080.frog...\n",
      "reading files from 081.frying-pan...\n",
      "reading files from 082.galaxy...\n",
      "reading files from 083.gas-pump...\n",
      "reading files from 084.giraffe...\n",
      "reading files from 085.goat...\n",
      "reading files from 086.golden-gate-bridge...\n",
      "reading files from 087.goldfish...\n",
      "reading files from 088.golf-ball...\n",
      "reading files from 089.goose...\n",
      "reading files from 090.gorilla...\n",
      "reading files from 091.grand-piano-101...\n",
      "reading files from 092.grapes...\n",
      "reading files from 093.grasshopper...\n",
      "reading files from 094.guitar-pick...\n",
      "reading files from 095.hamburger...\n",
      "reading files from 096.hammock...\n",
      "reading files from 097.harmonica...\n",
      "reading files from 098.harp...\n",
      "reading files from 099.harpsichord...\n",
      "reading files from 100.hawksbill-101...\n",
      "reading files from 101.head-phones...\n",
      "reading files from 102.helicopter-101...\n",
      "reading files from 103.hibiscus...\n",
      "reading files from 104.homer-simpson...\n",
      "reading files from 105.horse...\n",
      "reading files from 106.horseshoe-crab...\n",
      "reading files from 107.hot-air-balloon...\n",
      "reading files from 108.hot-dog...\n",
      "reading files from 109.hot-tub...\n",
      "reading files from 110.hourglass...\n",
      "reading files from 111.house-fly...\n",
      "reading files from 112.human-skeleton...\n",
      "reading files from 113.hummingbird...\n",
      "reading files from 114.ibis-101...\n",
      "reading files from 115.ice-cream-cone...\n",
      "reading files from 116.iguana...\n",
      "reading files from 117.ipod...\n",
      "reading files from 118.iris...\n",
      "reading files from 119.jesus-christ...\n",
      "reading files from 120.joy-stick...\n",
      "reading files from 121.kangaroo-101...\n",
      "reading files from 122.kayak...\n",
      "reading files from 123.ketch-101...\n",
      "reading files from 124.killer-whale...\n",
      "reading files from 125.knife...\n",
      "reading files from 126.ladder...\n",
      "reading files from 127.laptop-101...\n",
      "reading files from 128.lathe...\n",
      "reading files from 129.leopards-101...\n",
      "reading files from 130.license-plate...\n",
      "reading files from 131.lightbulb...\n",
      "reading files from 132.light-house...\n",
      "reading files from 133.lightning...\n",
      "reading files from 134.llama-101...\n",
      "reading files from 135.mailbox...\n",
      "reading files from 136.mandolin...\n",
      "reading files from 137.mars...\n",
      "reading files from 138.mattress...\n",
      "reading files from 139.megaphone...\n",
      "reading files from 140.menorah-101...\n",
      "reading files from 141.microscope...\n",
      "reading files from 142.microwave...\n",
      "reading files from 143.minaret...\n",
      "reading files from 144.minotaur...\n",
      "reading files from 145.motorbikes-101...\n",
      "reading files from 146.mountain-bike...\n",
      "reading files from 147.mushroom...\n",
      "reading files from 148.mussels...\n",
      "reading files from 149.necktie...\n",
      "reading files from 150.octopus...\n",
      "reading files from 151.ostrich...\n",
      "reading files from 152.owl...\n",
      "reading files from 153.palm-pilot...\n",
      "reading files from 154.palm-tree...\n",
      "reading files from 155.paperclip...\n",
      "reading files from 156.paper-shredder...\n",
      "reading files from 157.pci-card...\n",
      "reading files from 158.penguin...\n",
      "reading files from 159.people...\n",
      "reading files from 160.pez-dispenser...\n",
      "reading files from 161.photocopier...\n",
      "reading files from 162.picnic-table...\n",
      "reading files from 163.playing-card...\n",
      "reading files from 164.porcupine...\n",
      "reading files from 165.pram...\n",
      "reading files from 166.praying-mantis...\n",
      "reading files from 167.pyramid...\n",
      "reading files from 168.raccoon...\n",
      "reading files from 169.radio-telescope...\n",
      "reading files from 170.rainbow...\n",
      "reading files from 171.refrigerator...\n",
      "reading files from 172.revolver-101...\n",
      "reading files from 173.rifle...\n",
      "reading files from 174.rotary-phone...\n",
      "reading files from 175.roulette-wheel...\n",
      "reading files from 176.saddle...\n",
      "reading files from 177.saturn...\n",
      "reading files from 178.school-bus...\n",
      "reading files from 179.scorpion-101...\n",
      "reading files from 180.screwdriver...\n",
      "reading files from 181.segway...\n",
      "reading files from 182.self-propelled-lawn-mower...\n",
      "reading files from 183.sextant...\n",
      "reading files from 184.sheet-music...\n",
      "reading files from 185.skateboard...\n",
      "reading files from 186.skunk...\n",
      "reading files from 187.skyscraper...\n",
      "reading files from 188.smokestack...\n",
      "reading files from 189.snail...\n",
      "reading files from 190.snake...\n",
      "reading files from 191.sneaker...\n",
      "reading files from 192.snowmobile...\n",
      "reading files from 193.soccer-ball...\n",
      "reading files from 194.socks...\n",
      "reading files from 195.soda-can...\n",
      "reading files from 196.spaghetti...\n",
      "reading files from 197.speed-boat...\n",
      "reading files from 198.spider...\n",
      "reading files from 199.spoon...\n",
      "reading files from 200.stained-glass...\n",
      "reading files from 201.starfish-101...\n",
      "reading files from 202.steering-wheel...\n",
      "reading files from 203.stirrups...\n",
      "reading files from 204.sunflower-101...\n",
      "reading files from 205.superman...\n",
      "reading files from 206.sushi...\n",
      "reading files from 207.swan...\n",
      "reading files from 208.swiss-army-knife...\n",
      "reading files from 209.sword...\n",
      "reading files from 210.syringe...\n",
      "reading files from 211.tambourine...\n",
      "reading files from 212.teapot...\n",
      "reading files from 213.teddy-bear...\n",
      "reading files from 214.teepee...\n",
      "reading files from 215.telephone-box...\n",
      "reading files from 216.tennis-ball...\n",
      "reading files from 217.tennis-court...\n",
      "reading files from 218.tennis-racket...\n",
      "reading files from 219.theodolite...\n",
      "reading files from 220.toaster...\n",
      "reading files from 221.tomato...\n",
      "reading files from 222.tombstone...\n",
      "reading files from 223.top-hat...\n",
      "reading files from 224.touring-bike...\n",
      "reading files from 225.tower-pisa...\n",
      "reading files from 226.traffic-light...\n",
      "reading files from 227.treadmill...\n",
      "reading files from 228.triceratops...\n",
      "reading files from 229.tricycle...\n",
      "reading files from 230.trilobite-101...\n",
      "reading files from 231.tripod...\n",
      "reading files from 232.t-shirt...\n",
      "reading files from 233.tuning-fork...\n",
      "reading files from 234.tweezer...\n",
      "reading files from 235.umbrella-101...\n",
      "reading files from 236.unicorn...\n",
      "reading files from 237.vcr...\n",
      "reading files from 238.video-projector...\n",
      "reading files from 239.washing-machine...\n",
      "reading files from 240.watch-101...\n",
      "reading files from 241.waterfall...\n",
      "reading files from 242.watermelon...\n",
      "reading files from 243.welding-mask...\n",
      "reading files from 244.wheelbarrow...\n",
      "reading files from 245.windmill...\n",
      "reading files from 246.wine-bottle...\n",
      "reading files from 247.xylophone...\n",
      "reading files from 248.yarmulke...\n",
      "reading files from 249.yo-yo...\n",
      "reading files from 250.zebra...\n",
      "reading files from 251.airplanes-101...\n",
      "reading files from 252.car-side-101...\n",
      "reading files from 253.faces-easy-101...\n",
      "reading files from 254.greyhound...\n",
      "reading files from 255.tennis-shoes...\n",
      "reading files from 256.toad...\n",
      "reading files from 257.clutter...\n"
     ]
    }
   ],
   "source": [
    "# data folder paths\n",
    "dir_train = os.path.join('data', 'train')\n",
    "dir_test = os.path.join('data', 'test')\n",
    "\n",
    "# collect train metadata\n",
    "train_metadata = []\n",
    "\n",
    "for root, subFolders, files in os.walk(dir_train):\n",
    "    for sf in subFolders:\n",
    "        print('reading files from {}...'.format(sf))\n",
    "        for f_name in os.listdir(os.path.join(dir_train, sf)):\n",
    "            if not f_name.startswith('.'):\n",
    "                temp = misc.imread(os.path.join(dir_train, sf, f_name)) # read image\n",
    "                # collect image metadata\n",
    "                image_metadata = []\n",
    "                image_metadata.extend([sf, f_name])\n",
    "                image_metadata.extend( list(temp.shape) if len(temp.shape) == 3\\\n",
    "                                      else [temp.shape[0], temp.shape[1], 1])\n",
    "                image_metadata.extend([temp.nbytes, temp.dtype])\n",
    "                # append image metadata to list\n",
    "                train_metadata.append(image_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>img_name</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>channels</th>\n",
       "      <th>byte_size</th>\n",
       "      <th>bit_depth</th>\n",
       "      <th>target</th>\n",
       "      <th>img_extension</th>\n",
       "      <th>category_name</th>\n",
       "      <th>img_resolution</th>\n",
       "      <th>img_shape</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001.ak47</td>\n",
       "      <td>001_0001.jpg</td>\n",
       "      <td>278</td>\n",
       "      <td>499</td>\n",
       "      <td>3</td>\n",
       "      <td>416166</td>\n",
       "      <td>uint8</td>\n",
       "      <td>1</td>\n",
       "      <td>jpg</td>\n",
       "      <td>ak47</td>\n",
       "      <td>499x278</td>\n",
       "      <td>horizontal</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001.ak47</td>\n",
       "      <td>001_0002.jpg</td>\n",
       "      <td>218</td>\n",
       "      <td>268</td>\n",
       "      <td>3</td>\n",
       "      <td>175272</td>\n",
       "      <td>uint8</td>\n",
       "      <td>1</td>\n",
       "      <td>jpg</td>\n",
       "      <td>ak47</td>\n",
       "      <td>268x218</td>\n",
       "      <td>horizontal</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001.ak47</td>\n",
       "      <td>001_0003.jpg</td>\n",
       "      <td>186</td>\n",
       "      <td>300</td>\n",
       "      <td>3</td>\n",
       "      <td>167400</td>\n",
       "      <td>uint8</td>\n",
       "      <td>1</td>\n",
       "      <td>jpg</td>\n",
       "      <td>ak47</td>\n",
       "      <td>300x186</td>\n",
       "      <td>horizontal</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001.ak47</td>\n",
       "      <td>001_0004.jpg</td>\n",
       "      <td>185</td>\n",
       "      <td>250</td>\n",
       "      <td>3</td>\n",
       "      <td>138750</td>\n",
       "      <td>uint8</td>\n",
       "      <td>1</td>\n",
       "      <td>jpg</td>\n",
       "      <td>ak47</td>\n",
       "      <td>250x185</td>\n",
       "      <td>horizontal</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001.ak47</td>\n",
       "      <td>001_0005.jpg</td>\n",
       "      <td>200</td>\n",
       "      <td>380</td>\n",
       "      <td>3</td>\n",
       "      <td>228000</td>\n",
       "      <td>uint8</td>\n",
       "      <td>1</td>\n",
       "      <td>jpg</td>\n",
       "      <td>ak47</td>\n",
       "      <td>380x200</td>\n",
       "      <td>horizontal</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category      img_name  height  width  channels  byte_size bit_depth  \\\n",
       "0  001.ak47  001_0001.jpg     278    499         3     416166     uint8   \n",
       "1  001.ak47  001_0002.jpg     218    268         3     175272     uint8   \n",
       "2  001.ak47  001_0003.jpg     186    300         3     167400     uint8   \n",
       "3  001.ak47  001_0004.jpg     185    250         3     138750     uint8   \n",
       "4  001.ak47  001_0005.jpg     200    380         3     228000     uint8   \n",
       "\n",
       "   target img_extension category_name img_resolution   img_shape    size  \n",
       "0       1           jpg          ak47        499x278  horizontal  medium  \n",
       "1       1           jpg          ak47        268x218  horizontal   small  \n",
       "2       1           jpg          ak47        300x186  horizontal   small  \n",
       "3       1           jpg          ak47        250x185  horizontal   small  \n",
       "4       1           jpg          ak47        380x200  horizontal  medium  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make it more tidy-looking\n",
    "train_metadata = pd.DataFrame(train_metadata)\n",
    "train_metadata.columns = ['category','img_name','height','width','channels','byte_size','bit_depth']\n",
    "\n",
    "# add some features =============================================================================\n",
    "# select integer target (1-257)\n",
    "train_metadata['target'] = train_metadata['category'].str.split('.').apply(lambda x: int(x[0]))\n",
    "# image extension\n",
    "train_metadata['img_extension'] = train_metadata['img_name'].str.split('.').apply(lambda x: x[1])\n",
    "# just name without category indexes\n",
    "train_metadata['category_name'] = \\\n",
    "    train_metadata['category'].str.split('.').apply(lambda x: x[1]).str.lower()\n",
    "# img resolution\n",
    "train_metadata['img_resolution'] = (\n",
    "    train_metadata['width'].astype(str) + 'x' + train_metadata['height'].astype(str)\n",
    ").astype('category')\n",
    "# img shape/orientation\n",
    "train_metadata['img_shape'] = train_metadata.apply(lambda r:\\\n",
    "    'horizontal' if r['height'] < r['width']\\\n",
    "    else 'vertical' if r['height'] > r['width']\\\n",
    "    else 'squared', axis=1)\n",
    "# img size class (small/medium/big)\n",
    "train_metadata['size'] = train_metadata.apply(lambda r:\\\n",
    "    'small' if r['height']*r['width'] <= 256**2\\\n",
    "    else 'medium' if r['height']*r['width'] <= 512**2\\\n",
    "    else 'big', axis=1)\n",
    "\n",
    "# let's have a look at it now\n",
    "train_metadata.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category          0\n",
       "img_name          0\n",
       "height            0\n",
       "width             0\n",
       "channels          0\n",
       "byte_size         0\n",
       "bit_depth         0\n",
       "target            0\n",
       "img_extension     0\n",
       "category_name     0\n",
       "img_resolution    0\n",
       "img_shape         0\n",
       "size              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for NaNs / missing values - all looks good\n",
    "train_metadata.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    22599\n",
       "1      298\n",
       "Name: channels, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for 1-channel (gray) images: there are about 300 grayscale images (1.3%)\n",
    "train_metadata.channels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jpg    22897\n",
      "Name: img_extension, dtype: int64\n",
      "uint8    22897\n",
      "Name: bit_depth, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check for file extensions/bit depth - only .JPG, 8 bit per channel (0-255)\n",
    "print(train_metadata.img_extension.value_counts())\n",
    "print(train_metadata.bit_depth.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most popular class - 257: 3.48%\n",
      "least popular class - 99: 0.22%\n"
     ]
    }
   ],
   "source": [
    "# check for class balance (for ideal balance -> 100%/257 = 0.389%)\n",
    "print('most popular class - {}: {:.2f}%'.format(\n",
    "    train_metadata.target.value_counts(normalize=True).argmax(),\n",
    "    train_metadata.target.value_counts(normalize=True).max()*100\n",
    "))\n",
    "print('least popular class - {}: {:.2f}%'.format(\n",
    "    train_metadata.target.value_counts(normalize=True).argmin(),\n",
    "    train_metadata.target.value_counts(normalize=True).min()*100\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640x480    2.432633\n",
       "200x200    1.528585\n",
       "300x225    0.952090\n",
       "300x300    0.873477\n",
       "400x300    0.860375\n",
       "Name: img_resolution, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top-5 common resolutions (in %)\n",
    "train_metadata.img_resolution.value_counts(normalize=True)[:5]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "horizontal    62.484168\n",
       "vertical      29.650173\n",
       "squared        7.865659\n",
       "Name: img_shape, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image orientation (in %)\n",
    "train_metadata.img_shape.value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medium    46.599991\n",
       "small     39.118662\n",
       "big       14.281347\n",
       "Name: size, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image size category (in %)\n",
    "train_metadata['size'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Image Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_sure_path_exists(path):\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path, ignore_errors=True)\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exception:\n",
    "        if exception.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "def data_handler(path_to_folder,\n",
    "                 subfolders=False,\n",
    "                 transformation_list=None, # TODO\n",
    "                 output_dimension=(256, 256),\n",
    "                 channels=3,\n",
    "                 save_as_path=None):\n",
    "    \n",
    "    # create folder for transformed data if not exists\n",
    "    if save_as_path is not None:\n",
    "        make_sure_path_exists(save_as_path)\n",
    "    \n",
    "    # for all images - process them and save\n",
    "    if subfolders: # subfolders for each img class\n",
    "        for root, subFolders, files in os.walk(path_to_folder):\n",
    "            for sf in subFolders:\n",
    "                print('Transforming files from {}...'.format(sf))\n",
    "                for f_name in os.listdir(os.path.join(path_to_folder, sf)):\n",
    "                    if not f_name.startswith('.'):\n",
    "                        temp = misc.imread(os.path.join(path_to_folder, sf, f_name)) # read image\n",
    "                        # if it's grayscale (1 channel) - then naively \"extend\" it to 3 (24bit depth)\n",
    "                        if len(temp.shape) == 2:\n",
    "                            temp = np.stack((temp,)*3)\n",
    "                        # save image...\n",
    "                        misc.imsave(\n",
    "                            os.path.join(save_as_path, f_name),\n",
    "                            arr=misc.imresize(temp, size=output_dimension, interp='bicubic')\n",
    "                        )\n",
    "    else: # flattened structure\n",
    "        print('Transforming files from {}...'.format(path_to_folder))\n",
    "        for f_name in os.listdir(path_to_folder):\n",
    "            if not f_name.startswith('.'):\n",
    "                temp = misc.imread(os.path.join(path_to_folder, f_name)) # read image\n",
    "                # if it's grayscale (1 channel) - then naively \"extend\" it to 3 (24bit depth)\n",
    "                if len(temp.shape) == 2:\n",
    "                    temp = np.stack((temp,)*3)\n",
    "                # save image...\n",
    "                misc.imsave(\n",
    "                    os.path.join(save_as_path, f_name),\n",
    "                    arr=misc.imresize(temp, size=output_dimension, interp='bicubic')\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming files from 001.ak47...\n",
      "Transforming files from 002.american-flag...\n",
      "Transforming files from 003.backpack...\n",
      "Transforming files from 004.baseball-bat...\n",
      "Transforming files from 005.baseball-glove...\n",
      "Transforming files from 006.basketball-hoop...\n",
      "Transforming files from 007.bat...\n",
      "Transforming files from 008.bathtub...\n",
      "Transforming files from 009.bear...\n",
      "Transforming files from 010.beer-mug...\n",
      "Transforming files from 011.billiards...\n",
      "Transforming files from 012.binoculars...\n",
      "Transforming files from 013.birdbath...\n",
      "Transforming files from 014.blimp...\n",
      "Transforming files from 015.bonsai-101...\n",
      "Transforming files from 016.boom-box...\n",
      "Transforming files from 017.bowling-ball...\n",
      "Transforming files from 018.bowling-pin...\n",
      "Transforming files from 019.boxing-glove...\n",
      "Transforming files from 020.brain-101...\n",
      "Transforming files from 021.breadmaker...\n",
      "Transforming files from 022.buddha-101...\n",
      "Transforming files from 023.bulldozer...\n",
      "Transforming files from 024.butterfly...\n",
      "Transforming files from 025.cactus...\n",
      "Transforming files from 026.cake...\n",
      "Transforming files from 027.calculator...\n",
      "Transforming files from 028.camel...\n",
      "Transforming files from 029.cannon...\n",
      "Transforming files from 030.canoe...\n",
      "Transforming files from 031.car-tire...\n",
      "Transforming files from 032.cartman...\n",
      "Transforming files from 033.cd...\n",
      "Transforming files from 034.centipede...\n",
      "Transforming files from 035.cereal-box...\n",
      "Transforming files from 036.chandelier-101...\n",
      "Transforming files from 037.chess-board...\n",
      "Transforming files from 038.chimp...\n",
      "Transforming files from 039.chopsticks...\n",
      "Transforming files from 040.cockroach...\n",
      "Transforming files from 041.coffee-mug...\n",
      "Transforming files from 042.coffin...\n",
      "Transforming files from 043.coin...\n",
      "Transforming files from 044.comet...\n",
      "Transforming files from 045.computer-keyboard...\n",
      "Transforming files from 046.computer-monitor...\n",
      "Transforming files from 047.computer-mouse...\n",
      "Transforming files from 048.conch...\n",
      "Transforming files from 049.cormorant...\n",
      "Transforming files from 050.covered-wagon...\n",
      "Transforming files from 051.cowboy-hat...\n",
      "Transforming files from 052.crab-101...\n",
      "Transforming files from 053.desk-globe...\n",
      "Transforming files from 054.diamond-ring...\n",
      "Transforming files from 055.dice...\n",
      "Transforming files from 056.dog...\n",
      "Transforming files from 057.dolphin-101...\n",
      "Transforming files from 058.doorknob...\n",
      "Transforming files from 059.drinking-straw...\n",
      "Transforming files from 060.duck...\n",
      "Transforming files from 061.dumb-bell...\n",
      "Transforming files from 062.eiffel-tower...\n",
      "Transforming files from 063.electric-guitar-101...\n",
      "Transforming files from 064.elephant-101...\n",
      "Transforming files from 065.elk...\n",
      "Transforming files from 066.ewer-101...\n",
      "Transforming files from 067.eyeglasses...\n",
      "Transforming files from 068.fern...\n",
      "Transforming files from 069.fighter-jet...\n",
      "Transforming files from 070.fire-extinguisher...\n",
      "Transforming files from 071.fire-hydrant...\n",
      "Transforming files from 072.fire-truck...\n",
      "Transforming files from 073.fireworks...\n",
      "Transforming files from 074.flashlight...\n",
      "Transforming files from 075.floppy-disk...\n",
      "Transforming files from 076.football-helmet...\n",
      "Transforming files from 077.french-horn...\n",
      "Transforming files from 078.fried-egg...\n",
      "Transforming files from 079.frisbee...\n",
      "Transforming files from 080.frog...\n",
      "Transforming files from 081.frying-pan...\n",
      "Transforming files from 082.galaxy...\n",
      "Transforming files from 083.gas-pump...\n",
      "Transforming files from 084.giraffe...\n",
      "Transforming files from 085.goat...\n",
      "Transforming files from 086.golden-gate-bridge...\n",
      "Transforming files from 087.goldfish...\n",
      "Transforming files from 088.golf-ball...\n",
      "Transforming files from 089.goose...\n",
      "Transforming files from 090.gorilla...\n",
      "Transforming files from 091.grand-piano-101...\n",
      "Transforming files from 092.grapes...\n",
      "Transforming files from 093.grasshopper...\n",
      "Transforming files from 094.guitar-pick...\n",
      "Transforming files from 095.hamburger...\n",
      "Transforming files from 096.hammock...\n",
      "Transforming files from 097.harmonica...\n",
      "Transforming files from 098.harp...\n",
      "Transforming files from 099.harpsichord...\n",
      "Transforming files from 100.hawksbill-101...\n",
      "Transforming files from 101.head-phones...\n",
      "Transforming files from 102.helicopter-101...\n",
      "Transforming files from 103.hibiscus...\n",
      "Transforming files from 104.homer-simpson...\n",
      "Transforming files from 105.horse...\n",
      "Transforming files from 106.horseshoe-crab...\n",
      "Transforming files from 107.hot-air-balloon...\n",
      "Transforming files from 108.hot-dog...\n",
      "Transforming files from 109.hot-tub...\n",
      "Transforming files from 110.hourglass...\n",
      "Transforming files from 111.house-fly...\n",
      "Transforming files from 112.human-skeleton...\n",
      "Transforming files from 113.hummingbird...\n",
      "Transforming files from 114.ibis-101...\n",
      "Transforming files from 115.ice-cream-cone...\n",
      "Transforming files from 116.iguana...\n",
      "Transforming files from 117.ipod...\n",
      "Transforming files from 118.iris...\n",
      "Transforming files from 119.jesus-christ...\n",
      "Transforming files from 120.joy-stick...\n",
      "Transforming files from 121.kangaroo-101...\n",
      "Transforming files from 122.kayak...\n",
      "Transforming files from 123.ketch-101...\n",
      "Transforming files from 124.killer-whale...\n",
      "Transforming files from 125.knife...\n",
      "Transforming files from 126.ladder...\n",
      "Transforming files from 127.laptop-101...\n",
      "Transforming files from 128.lathe...\n",
      "Transforming files from 129.leopards-101...\n",
      "Transforming files from 130.license-plate...\n",
      "Transforming files from 131.lightbulb...\n",
      "Transforming files from 132.light-house...\n",
      "Transforming files from 133.lightning...\n",
      "Transforming files from 134.llama-101...\n",
      "Transforming files from 135.mailbox...\n",
      "Transforming files from 136.mandolin...\n",
      "Transforming files from 137.mars...\n",
      "Transforming files from 138.mattress...\n",
      "Transforming files from 139.megaphone...\n",
      "Transforming files from 140.menorah-101...\n",
      "Transforming files from 141.microscope...\n",
      "Transforming files from 142.microwave...\n",
      "Transforming files from 143.minaret...\n",
      "Transforming files from 144.minotaur...\n",
      "Transforming files from 145.motorbikes-101...\n",
      "Transforming files from 146.mountain-bike...\n",
      "Transforming files from 147.mushroom...\n",
      "Transforming files from 148.mussels...\n",
      "Transforming files from 149.necktie...\n",
      "Transforming files from 150.octopus...\n",
      "Transforming files from 151.ostrich...\n",
      "Transforming files from 152.owl...\n",
      "Transforming files from 153.palm-pilot...\n",
      "Transforming files from 154.palm-tree...\n",
      "Transforming files from 155.paperclip...\n",
      "Transforming files from 156.paper-shredder...\n",
      "Transforming files from 157.pci-card...\n",
      "Transforming files from 158.penguin...\n",
      "Transforming files from 159.people...\n",
      "Transforming files from 160.pez-dispenser...\n",
      "Transforming files from 161.photocopier...\n",
      "Transforming files from 162.picnic-table...\n",
      "Transforming files from 163.playing-card...\n",
      "Transforming files from 164.porcupine...\n",
      "Transforming files from 165.pram...\n",
      "Transforming files from 166.praying-mantis...\n",
      "Transforming files from 167.pyramid...\n",
      "Transforming files from 168.raccoon...\n",
      "Transforming files from 169.radio-telescope...\n",
      "Transforming files from 170.rainbow...\n",
      "Transforming files from 171.refrigerator...\n",
      "Transforming files from 172.revolver-101...\n",
      "Transforming files from 173.rifle...\n",
      "Transforming files from 174.rotary-phone...\n",
      "Transforming files from 175.roulette-wheel...\n",
      "Transforming files from 176.saddle...\n",
      "Transforming files from 177.saturn...\n",
      "Transforming files from 178.school-bus...\n",
      "Transforming files from 179.scorpion-101...\n",
      "Transforming files from 180.screwdriver...\n",
      "Transforming files from 181.segway...\n",
      "Transforming files from 182.self-propelled-lawn-mower...\n",
      "Transforming files from 183.sextant...\n",
      "Transforming files from 184.sheet-music...\n",
      "Transforming files from 185.skateboard...\n",
      "Transforming files from 186.skunk...\n",
      "Transforming files from 187.skyscraper...\n",
      "Transforming files from 188.smokestack...\n",
      "Transforming files from 189.snail...\n",
      "Transforming files from 190.snake...\n",
      "Transforming files from 191.sneaker...\n",
      "Transforming files from 192.snowmobile...\n",
      "Transforming files from 193.soccer-ball...\n",
      "Transforming files from 194.socks...\n",
      "Transforming files from 195.soda-can...\n",
      "Transforming files from 196.spaghetti...\n",
      "Transforming files from 197.speed-boat...\n",
      "Transforming files from 198.spider...\n",
      "Transforming files from 199.spoon...\n",
      "Transforming files from 200.stained-glass...\n",
      "Transforming files from 201.starfish-101...\n",
      "Transforming files from 202.steering-wheel...\n",
      "Transforming files from 203.stirrups...\n",
      "Transforming files from 204.sunflower-101...\n",
      "Transforming files from 205.superman...\n",
      "Transforming files from 206.sushi...\n",
      "Transforming files from 207.swan...\n",
      "Transforming files from 208.swiss-army-knife...\n",
      "Transforming files from 209.sword...\n",
      "Transforming files from 210.syringe...\n",
      "Transforming files from 211.tambourine...\n",
      "Transforming files from 212.teapot...\n",
      "Transforming files from 213.teddy-bear...\n",
      "Transforming files from 214.teepee...\n",
      "Transforming files from 215.telephone-box...\n",
      "Transforming files from 216.tennis-ball...\n",
      "Transforming files from 217.tennis-court...\n",
      "Transforming files from 218.tennis-racket...\n",
      "Transforming files from 219.theodolite...\n",
      "Transforming files from 220.toaster...\n",
      "Transforming files from 221.tomato...\n",
      "Transforming files from 222.tombstone...\n",
      "Transforming files from 223.top-hat...\n",
      "Transforming files from 224.touring-bike...\n",
      "Transforming files from 225.tower-pisa...\n",
      "Transforming files from 226.traffic-light...\n",
      "Transforming files from 227.treadmill...\n",
      "Transforming files from 228.triceratops...\n",
      "Transforming files from 229.tricycle...\n",
      "Transforming files from 230.trilobite-101...\n",
      "Transforming files from 231.tripod...\n",
      "Transforming files from 232.t-shirt...\n",
      "Transforming files from 233.tuning-fork...\n",
      "Transforming files from 234.tweezer...\n",
      "Transforming files from 235.umbrella-101...\n",
      "Transforming files from 236.unicorn...\n",
      "Transforming files from 237.vcr...\n",
      "Transforming files from 238.video-projector...\n",
      "Transforming files from 239.washing-machine...\n",
      "Transforming files from 240.watch-101...\n",
      "Transforming files from 241.waterfall...\n",
      "Transforming files from 242.watermelon...\n",
      "Transforming files from 243.welding-mask...\n",
      "Transforming files from 244.wheelbarrow...\n",
      "Transforming files from 245.windmill...\n",
      "Transforming files from 246.wine-bottle...\n",
      "Transforming files from 247.xylophone...\n",
      "Transforming files from 248.yarmulke...\n",
      "Transforming files from 249.yo-yo...\n",
      "Transforming files from 250.zebra...\n",
      "Transforming files from 251.airplanes-101...\n",
      "Transforming files from 252.car-side-101...\n",
      "Transforming files from 253.faces-easy-101...\n",
      "Transforming files from 254.greyhound...\n",
      "Transforming files from 255.tennis-shoes...\n",
      "Transforming files from 256.toad...\n",
      "Transforming files from 257.clutter...\n",
      "Wall time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# directory to hold flattened data (without subfolders)\n",
    "dir_train_transformed = os.path.join('data', 'train_transformed')\n",
    "\n",
    "# transform data and re-save it in transformed folder\n",
    "data_handler(\n",
    "    path_to_folder=dir_train,\n",
    "    subfolders=True,\n",
    "    output_dimension=(64, 64),\n",
    "    save_as_path=dir_train_transformed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0/ 22897 images loaded\n",
      "  5000/ 22897 images loaded\n",
      " 10000/ 22897 images loaded\n",
      " 15000/ 22897 images loaded\n",
      " 20000/ 22897 images loaded\n",
      "Wall time: 47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load data in flattened form\n",
    "\n",
    "# img parameters\n",
    "img_shape = (64, 64)\n",
    "channels = 3\n",
    "img_shape_flattened = img_shape[0] * img_shape[1] * channels\n",
    "img_qty = len(train_metadata)\n",
    "img_dtype = np.int8\n",
    "\n",
    "# initialize X,y\n",
    "X = np.empty(shape=(img_qty, img_shape_flattened), dtype=img_dtype)\n",
    "y = np.empty(shape=(img_qty,), dtype=np.uint16)\n",
    "\n",
    "# read images\n",
    "for i,f_name in enumerate(os.listdir(dir_train_transformed)):\n",
    "    if i % 5000 == 0:\n",
    "        print('{:6d}/{:6d} images loaded'.format(i, img_qty))\n",
    "    \n",
    "    img_path = os.path.join(dir_train_transformed, f_name)\n",
    "    X[i, :] = misc.imread(img_path).flatten('C') # since img is np.ndarray, flatten in row-style\n",
    "    y[i] = train_metadata[train_metadata.img_name == f_name].target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X size in Mb: 268.32\n",
      "y size in Mb: 0.04\n",
      "(22897, 12288) (22897,)\n"
     ]
    }
   ],
   "source": [
    "# check memory consumption\n",
    "print('X size in Mb: {:.2f}'.format(X.nbytes/2**20))\n",
    "print('y size in Mb: {:.2f}'.format(y.nbytes/2**20))\n",
    "# check shapes\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, \n",
    "                                                  test_size=0.4,\n",
    "                                                  stratify=y, # to preserve class balance\n",
    "                                                  random_state=42 # for reproducibility\n",
    "                                                 )\n",
    "# drop redundant matrices\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\neural-networks\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]Accuracy on train: 0.1313873926335711\n",
      "Accuracy on validation: 0.06594606398078393\n",
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression  # baseline classifier\n",
    "from sklearn.preprocessing import StandardScaler # for data standartization ~ N(0,1)\n",
    "from sklearn.decomposition import PCA # to reduce training columns\n",
    "from sklearn.pipeline import Pipeline # to \"glue\" model steps/components together\n",
    "from sklearn.metrics import accuracy_score # to test quality of the classifier\n",
    "\n",
    "\n",
    "# define components\n",
    "sc = StandardScaler()\n",
    "pca = PCA(n_components=50, random_state=42)\n",
    "svc = LogisticRegression(random_state=42, \n",
    "                         C=0.1, \n",
    "                         max_iter=50, \n",
    "                         verbose=2, \n",
    "                         n_jobs=-1,\n",
    "                         class_weight='balanced'\n",
    "                        )\n",
    "\n",
    "# create model\n",
    "model = Pipeline(\n",
    "    (\n",
    "        ('scaler', sc), # data standartization ~ N(0,1)\n",
    "        ('dim_reduction', pca), # firstly - reduce dimension \n",
    "        ('classifier', svc) # then - classify (multiclass)\n",
    "    )\n",
    ")\n",
    "\n",
    "# train model\n",
    "model.fit(X_train, y_train)\n",
    "print('Accuracy on train: {}'.format(accuracy_score(y_train, model.predict(X_train))))\n",
    "# predict validation\n",
    "print('Accuracy on validation: {}'.format(accuracy_score(y_val, model.predict(X_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming files from data\\test...\n",
      "Wall time: 46.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# directory to hold flattened test data (without subfolders)\n",
    "dir_test_transformed = os.path.join('data', 'test_transformed')\n",
    "\n",
    "# transform test data and re-save it in transformed folder\n",
    "data_handler(\n",
    "    path_to_folder=dir_test,\n",
    "    subfolders=False,\n",
    "    output_dimension=(64,64),\n",
    "    save_as_path=dir_test_transformed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0/  7710 images loaded\n",
      "  2500/  7710 images loaded\n",
      "  5000/  7710 images loaded\n",
      "  7500/  7710 images loaded\n"
     ]
    }
   ],
   "source": [
    "# form X_test matrix\n",
    "test_img_qty = len(os.listdir(dir_test_transformed))\n",
    "X_test = np.empty(shape=(test_img_qty, img_shape_flattened), dtype=img_dtype)\n",
    "\n",
    "# read images\n",
    "for i,f_name in enumerate(os.listdir(dir_test_transformed)):\n",
    "    if i % 2500 == 0:\n",
    "        print('{:6d}/{:6d} images loaded'.format(i, test_img_qty))\n",
    "    \n",
    "    img_path = os.path.join(dir_test_transformed, f_name)\n",
    "    X_test[i, :] = misc.imread(img_path).flatten('C') # since img is np.ndarray, flatten in row-style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\neural-networks\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# make prediction\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# create submission dataframe (in specified kaggle format)\n",
    "sub = pd.DataFrame(\n",
    "    data=[row for row in zip(os.listdir(dir_test_transformed), y_test_pred.astype(int).tolist())],\n",
    "    columns=['image', 'class']\n",
    ")\n",
    "\n",
    "# save it to .csv\n",
    "sub.to_csv('lr_baseline.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:neural-networks]",
   "language": "python",
   "name": "conda-env-neural-networks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
